{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## type of tensor\n",
    "With the exception of tf.Variable, the value of a tensor is immutable, which means that in the context of a single execution tensors only have a single value. However, evaluating the same tensor twice can return different values; for example that tensor can be the result of reading data from disk, or generating a random number.\n",
    "- tf.Variable: Unlike tf.Tensor objects, a tf.Variable exists outside the context of a single session.run call\n",
    "- tf.constant\n",
    "- tf.placeholder\n",
    "- tf.SparseTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank of tensor\n",
    "- 0:\tScalar (magnitude only)\n",
    "- 1:\tVector (magnitude and direction)\n",
    "- 2:\tMatrix (table of numbers)\n",
    "- 3:\t3-Tensor (cube of numbers)\n",
    "- n:\tn-Tensor (you get the idea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that when the initializer is a tf.Tensor you should not specify the variable's shape, as the shape of the initializer tensor will be used.\n",
    "other_variable = tf.get_variable(\"other_variable\", dtype=tf.int32,\n",
    "  initializer=tf.constant([23, 42]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## type of variable\n",
    "- tf.Variable & tf.get_variable is trainable\n",
    "- tf.placeholder is not a variable so not trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder:0\n",
      "ph:0\n",
      "ph_1:0\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Tensor(\"ph_1:0\", shape=(2, 3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "v1 = tf.placeholder(tf.float32, shape=[2,3,4])\n",
    "print (v1.name)\n",
    "v1 = tf.placeholder(tf.float32, shape=[2,3,4], name='ph')\n",
    "print (v1.name)\n",
    "v1 = tf.placeholder(tf.float32, shape=[2,3,4], name='ph')\n",
    "print (v1.name)\n",
    "print (type(v1))\n",
    "print (v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable:0\n",
      "V:0\n",
      "V_1:0\n",
      "<class 'tensorflow.python.ops.variables.Variable'>\n",
      "<tf.Variable 'V_1:0' shape=(2,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "v2 = tf.Variable([1,2], dtype=tf.float32)\n",
    "print (v2.name)\n",
    "v2 = tf.Variable([1,2], dtype=tf.float32, name='V')\n",
    "print (v2.name)\n",
    "v2 = tf.Variable([1,2], dtype=tf.float32, name='V')\n",
    "print (v2.name)\n",
    "print (type(v2))\n",
    "print (v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.get_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gv:0\n",
      "<class 'tensorflow.python.ops.variables.Variable'>\n",
      "<tf.Variable 'gv:0' shape=() dtype=float32_ref>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable gv already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-21-c42a39489826>\", line 1, in <module>\n    v3 = tf.get_variable(name='gv', shape=[])\n  File \"/Users/jojotenya/.virtualenvs/dl/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/Users/jojotenya/.virtualenvs/dl/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c42a39489826>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mv4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1295\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1298\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1299\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/.virtualenvs/dl/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1091\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/.virtualenvs/dl/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    437\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m~/.virtualenvs/dl/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    406\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    745\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 747\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    748\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable gv already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-21-c42a39489826>\", line 1, in <module>\n    v3 = tf.get_variable(name='gv', shape=[])\n  File \"/Users/jojotenya/.virtualenvs/dl/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/Users/jojotenya/.virtualenvs/dl/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "v3 = tf.get_variable(name='gv', shape=[])  \n",
    "print (v3.name)\n",
    "print (type(v3))\n",
    "print (v3)\n",
    "v4 = tf.get_variable(name='gv', shape=[2])\n",
    "print (v4.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "<tf.Variable 'Variable:0' shape=(2,) dtype=float32_ref>\n",
      "<tf.Variable 'V:0' shape=(2,) dtype=float32_ref>\n",
      "<tf.Variable 'V_1:0' shape=(2,) dtype=float32_ref>\n",
      "<tf.Variable 'gv:0' shape=() dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "vs = tf.trainable_variables()\n",
    "print (len(vs))\n",
    "for v in vs:\n",
    "    print (v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## name_scope vs variable_scope\n",
    "- 由下v3可知，name_scope並不會對tf.get_variable()創建的變量有命名空間上的影響，只有variable_scope會。variable_scope才會對tf.get_variable()的命名空間產生影響。\n",
    "- name_scope讓我們方變得管理命名空間，variable_scope實現變數共享。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1.name:  nsc1/v1:0\n",
      "v2.name:  nsc1/vsc1/v2:0\n",
      "v3.name:  vsc1/v3:0\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('nsc1'):\n",
    "    v1 = tf.Variable([1], name='v1')\n",
    "    with tf.variable_scope('vsc1'):\n",
    "        v2 = tf.Variable([1], name='v2')\n",
    "        v3 = tf.get_variable(name='v3', shape=[])\n",
    "print ('v1.name: ', v1.name)\n",
    "print ('v2.name: ', v2.name)\n",
    "print ('v3.name: ', v3.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v4.name:  nsc1_1/v4:0\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('nsc1'):\n",
    "    v4 = tf.Variable([1], name='v4')\n",
    "print ('v4.name: ', v4.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without any scope control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 trainable_variables in the Graph: \n",
      "<tf.Variable 'conv1/weights:0' shape=(5, 5, 32, 32) dtype=float32_ref>\n",
      "<tf.Variable 'conv1/biases:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'conv2/weights:0' shape=(5, 5, 32, 32) dtype=float32_ref>\n",
      "<tf.Variable 'conv2/biases:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'conv1_weights:0' shape=(5, 5, 32, 32) dtype=float32_ref>\n",
      "<tf.Variable 'conv1_biases:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'conv2_weights:0' shape=(5, 5, 32, 32) dtype=float32_ref>\n",
      "<tf.Variable 'conv2_biases:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'conv1_weights_1:0' shape=(5, 5, 32, 32) dtype=float32_ref>\n",
      "<tf.Variable 'conv1_biases_1:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'conv2_weights_1:0' shape=(5, 5, 32, 32) dtype=float32_ref>\n",
      "<tf.Variable 'conv2_biases_1:0' shape=(32,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "def my_image_filter():\n",
    "    conv1_weights = tf.Variable(tf.random_normal([5, 5, 32, 32]),\n",
    "        name=\"conv1_weights\")\n",
    "    conv1_biases = tf.Variable(tf.zeros([32]), name=\"conv1_biases\")\n",
    "    conv2_weights = tf.Variable(tf.random_normal([5, 5, 32, 32]),\n",
    "        name=\"conv2_weights\")\n",
    "    conv2_biases = tf.Variable(tf.zeros([32]), name=\"conv2_biases\")\n",
    "    return None\n",
    "\n",
    "result1 = my_image_filter()\n",
    "result2 = my_image_filter()\n",
    "vs = tf.trainable_variables()\n",
    "print ('There are %d trainable_variables in the Graph: ' % len(vs))\n",
    "for v in vs:\n",
    "    print (v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with variable_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 trainable_variables in the Graph: \n",
      "<tf.Variable 'image_filters/conv1/weights:0' shape=(5, 5, 32, 32) dtype=float32_ref>\n",
      "<tf.Variable 'image_filters/conv1/biases:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'image_filters/conv2/weights:0' shape=(5, 5, 32, 32) dtype=float32_ref>\n",
      "<tf.Variable 'image_filters/conv2/biases:0' shape=(32,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "def conv_relu(kernel_shape, bias_shape):\n",
    "    # Create variable named \"weights\".\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape, initializer=tf.random_normal_initializer())\n",
    "    # Create variable named \"biases\".\n",
    "    biases = tf.get_variable(\"biases\", bias_shape, initializer=tf.constant_initializer(0.0))\n",
    "    return None\n",
    "\n",
    "\n",
    "def my_image_filter():\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        # Variables created here will be named \"conv1/weights\", \"conv1/biases\".\n",
    "        relu1 = conv_relu([5, 5, 32, 32], [32])\n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        # Variables created here will be named \"conv2/weights\", \"conv2/biases\".\n",
    "        return conv_relu( [5, 5, 32, 32], [32])\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.variable_scope(\"image_filters\") as scope:\n",
    "    result1 = my_image_filter()\n",
    "    scope.reuse_variables()\n",
    "    result2 = my_image_filter()\n",
    "\n",
    "vs = tf.trainable_variables()\n",
    "print ('There are %d trainable_variables in the Graph: ' % len(vs))\n",
    "for v in vs:\n",
    "    print (v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with name_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'reuse_variables'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-6ff2a96081c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mresult1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_image_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mresult2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_image_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'reuse_variables'"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.name_scope(\"image_filters\") as scope:\n",
    "    result1 = my_image_filter()\n",
    "    print(type(scope))\n",
    "    scope.reuse_variables()\n",
    "    result2 = my_image_filter()\n",
    "\n",
    "vs = tf.trainable_variables()\n",
    "print ('There are %d trainable_variables in the Graph: ' % len(vs))\n",
    "for v in vs:\n",
    "    print (v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name scope:  nsc_1/\n",
      "type of name scope:  <class 'str'>\n",
      "variable scope:  <tensorflow.python.ops.variable_scope.VariableScope object at 0x1146236d8>\n",
      "type of variable scope:  <class 'tensorflow.python.ops.variable_scope.VariableScope'>\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"nsc\") as scope:\n",
    "    print('name scope: ',scope)\n",
    "    print('type of name scope: ',type(scope))\n",
    "\n",
    "with tf.variable_scope(\"vsc\") as scope:\n",
    "    print('variable scope: ',scope)\n",
    "    print('type of variable scope: ',type(scope))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.name_scope'> <class 'tensorflow.python.ops.variable_scope.variable_scope'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tf.name_scope('nsc')),type(tf.variable_scope('vsc')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## name_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_0:  Tensor(\"c_2:0\", shape=(), dtype=int32)\n",
      "c_1:  Tensor(\"c_3:0\", shape=(), dtype=int32)\n",
      "c_2:  Tensor(\"outer_1/c:0\", shape=(), dtype=int32)\n",
      "c_3:  Tensor(\"outer_1/inner/c:0\", shape=(), dtype=int32)\n",
      "c_4:  Tensor(\"outer_1/c_1:0\", shape=(), dtype=int32)\n",
      "c_5:  Tensor(\"outer_1/inner_1/c:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "c_0 = tf.constant(0, name=\"c\")  # => operation named \"c\"\n",
    "print('c_0: ',c_0)\n",
    "\n",
    "# Already-used names will be \"uniquified\".\n",
    "c_1 = tf.constant(2, name=\"c\")  # => operation named \"c_1\"\n",
    "print('c_1: ',c_1)\n",
    "\n",
    "# Name scopes add a prefix to all operations created in the same context.\n",
    "with tf.name_scope(\"outer\"):\n",
    "    c_2 = tf.constant(2, name=\"c\")  # => operation named \"outer/c\"\n",
    "    print('c_2: ',c_2)\n",
    "\n",
    "    # Name scopes nest like paths in a hierarchical file system.\n",
    "    with tf.name_scope(\"inner\"):\n",
    "        c_3 = tf.constant(3, name=\"c\")  # => operation named \"outer/inner/c\"\n",
    "        print('c_3: ',c_3)\n",
    "\n",
    "    # Exiting a name scope context will return to the previous prefix.\n",
    "    c_4 = tf.constant(4, name=\"c\")  # => operation named \"outer/c_1\"\n",
    "    print('c_4: ',c_4)\n",
    "\n",
    "    # Already-used name scopes will be \"uniquified\".\n",
    "    with tf.name_scope(\"inner\"):\n",
    "        c_5 = tf.constant(5, name=\"c\")  # => operation named \"outer/inner_1/c\"\n",
    "        print('c_5: ',c_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has to do with representation of tensors in underlying API. A tensor is a value associated with output of some op. In case of variables, there's a Variable op with one output. An op can have more than one output, so those tensors get referenced to as <op>:0, <op>:1 etc. For instance if you use tf.nn.top_k, there are two values created by this op, so you may see TopKV2:0 and TopKV2:1\n",
    "```\n",
    "a,b=tf.nn.top_k([1], 1)\n",
    "print a.name # => 'TopKV2:0'\n",
    "print b.name # => 'TopKV2:1'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variable collection\n",
    "named list of tensors or other objects, such as tf.Variable instances: disconnected part of a TensorFlow program might want to create variables, collection is a way to access them.\n",
    "\n",
    "**By default, tf.Variable are put in the following two collecitons:**\n",
    "- tf.GraphKeys.GLOBAL_VARIABLES\n",
    "- tf.GraphKeys.TRAINABLE_VARIABLES\n",
    "\n",
    "**If don't want a variable be trainable, put it into:**\n",
    "- tf.GraphKeys.LOCAL_VARIABLES\n",
    "```python\n",
    "my_local = tf.get_variable(\"my_local\", shape=(),\n",
    "collections=[tf.GraphKeys.LOCAL_VARIABLES])\n",
    "```\n",
    "(more commonly, we just set the parameter \"trainable\" to False)\n",
    "\n",
    "**Customized collections:**\n",
    "- tf.add_to_collection\n",
    "```python\n",
    "tf.add_to_collection(\"my_collection\", my_local)\n",
    "```\n",
    "\n",
    "**Get target collection:**\n",
    "- tf.get_collection\n",
    "```python\n",
    "tf.get_collection(\"my_collection\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variable initialization\n",
    "after execute: sess.run(tf.global_variable_initializer()), it will return a single operation responsible for initializing all variables in the tf.GraphKeys.GLOBAL_VARIABLES collection\n",
    "\n",
    "**initialize one varibale**: sess.run(your_variable.initializer)\n",
    "\n",
    "**check if there are variables uninitialized**: sess.run(tf.report_uninitialized_variables())\n",
    "\n",
    "**if the initial value of a variable depends on another variable's value**: variable.initialized_value() \n",
    "<table>\n",
    "<tr><td>\n",
    "v = tf.get_variable(\"v\", shape=(), initializer=tf.zeros_initializer())\n",
    "</td>\n",
    "<td>\n",
    "w = tf.get_variable(\"w\", initializer=v.initialized_value() + 1)\n",
    "</td></tr>\n",
    "</table>\n",
    "(by default tf.global_variables_initializer does not specify the order in which variables are initialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor-like object\n",
    "- tf.Tensor\n",
    "- tf.Variable\n",
    "- numpy.ndarray\n",
    "- list (and lists of tensor-like objects)\n",
    "- Scalar Python types: bool, float, int, str\n",
    "\n",
    "**tf.convert_to_tensor** can be used in the case of feeding huge ndarray repeatedly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.session\n",
    "\n",
    "**functionality:**\n",
    "- connection between the client program\n",
    "- provides access to devices in the local machine, and remote devices using the distributed TensorFlow runtime\n",
    "- caches information about your tf.Graph\n",
    "\n",
    "```python\n",
    "# Create a default in-process session.\n",
    "with tf.Session() as sess:\n",
    "  # ...\n",
    "\n",
    "# Create a remote session.\n",
    "with tf.Session(\"grpc://example.org:2222\"):\n",
    "  # ...\n",
    "```\n",
    "\n",
    "**init:**\n",
    "- target: use which devices, local machine by default, if remote, specify a grpc:// URL \n",
    "- graph: \n",
    "- config: behaviour of sess\n",
    " - gpu_options.allow_growth: True --> GPU memory allocator so that it gradually increases the amount of memory allocated, rather than allocating most of the memory at startup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trace graph execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[node {\n",
      "  name: \"MatMul/a\"\n",
      "  op: \"Const\"\n",
      "  device: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "  attr {\n",
      "    key: \"dtype\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"value\"\n",
      "    value {\n",
      "      tensor {\n",
      "        dtype: DT_FLOAT\n",
      "        tensor_shape {\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "        }\n",
      "        tensor_content: \"\\000\\000\\024B\\000\\000\\270\\301\\000\\000\\200?\\000\\000\\200@\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"random_uniform/shape\"\n",
      "  op: \"Const\"\n",
      "  device: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "  attr {\n",
      "    key: \"dtype\"\n",
      "    value {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"value\"\n",
      "    value {\n",
      "      tensor {\n",
      "        dtype: DT_INT32\n",
      "        tensor_shape {\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "        }\n",
      "        tensor_content: \"\\002\\000\\000\\000\\002\\000\\000\\000\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"random_uniform/RandomUniform\"\n",
      "  op: \"RandomUniform\"\n",
      "  input: \"random_uniform/shape\"\n",
      "  device: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "  attr {\n",
      "    key: \"T\"\n",
      "    value {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"dtype\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"seed\"\n",
      "    value {\n",
      "      i: 0\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"seed2\"\n",
      "    value {\n",
      "      i: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"random_uniform/mul\"\n",
      "  op: \"Snapshot\"\n",
      "  input: \"random_uniform/RandomUniform\"\n",
      "  device: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "  attr {\n",
      "    key: \"T\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"random_uniform\"\n",
      "  op: \"Snapshot\"\n",
      "  input: \"random_uniform/mul\"\n",
      "  device: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "  attr {\n",
      "    key: \"T\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"MatMul\"\n",
      "  op: \"MatMul\"\n",
      "  input: \"MatMul/a\"\n",
      "  input: \"random_uniform\"\n",
      "  device: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "  attr {\n",
      "    key: \"T\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"transpose_a\"\n",
      "    value {\n",
      "      b: false\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"transpose_b\"\n",
      "    value {\n",
      "      b: false\n",
      "    }\n",
      "  }\n",
      "}\n",
      "node {\n",
      "  name: \"_retval_MatMul_0_0\"\n",
      "  op: \"_Retval\"\n",
      "  input: \"MatMul\"\n",
      "  device: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "  attr {\n",
      "    key: \"T\"\n",
      "    value {\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "  attr {\n",
      "    key: \"index\"\n",
      "    value {\n",
      "      i: 0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "library {\n",
      "}\n",
      "versions {\n",
      "  producer: 26\n",
      "}\n",
      "]\n",
      "dev_stats {\n",
      "  device: \"/job:localhost/replica:0/task:0/device:CPU:0\"\n",
      "  node_stats {\n",
      "    node_name: \"_SOURCE\"\n",
      "    all_start_micros: 1527656218974019\n",
      "    op_start_rel_micros: 446\n",
      "    op_end_rel_micros: 447\n",
      "    all_end_rel_micros: 1063\n",
      "    memory {\n",
      "      allocator_name: \"cpu\"\n",
      "    }\n",
      "    timeline_label: \"_SOURCE = NoOp()\"\n",
      "    scheduled_micros: 1527656218973980\n",
      "    memory_stats {\n",
      "    }\n",
      "  }\n",
      "  node_stats {\n",
      "    node_name: \"MatMul/a\"\n",
      "    all_start_micros: 1527656218975096\n",
      "    op_start_rel_micros: 3\n",
      "    op_end_rel_micros: 12\n",
      "    all_end_rel_micros: 18\n",
      "    memory {\n",
      "      allocator_name: \"cpu\"\n",
      "    }\n",
      "    output {\n",
      "      tensor_description {\n",
      "        dtype: DT_FLOAT\n",
      "        shape {\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "        }\n",
      "        allocation_description {\n",
      "          requested_bytes: 16\n",
      "          allocator_name: \"cpu\"\n",
      "          ptr: 140473296640512\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    timeline_label: \"MatMul/a = Const()\"\n",
      "    scheduled_micros: 1527656218975082\n",
      "    memory_stats {\n",
      "      persistent_memory_size: 16\n",
      "    }\n",
      "  }\n",
      "  node_stats {\n",
      "    node_name: \"random_uniform/shape\"\n",
      "    all_start_micros: 1527656218975116\n",
      "    op_start_rel_micros: 1\n",
      "    op_end_rel_micros: 3\n",
      "    all_end_rel_micros: 5\n",
      "    memory {\n",
      "      allocator_name: \"cpu\"\n",
      "    }\n",
      "    output {\n",
      "      tensor_description {\n",
      "        dtype: DT_INT32\n",
      "        shape {\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "        }\n",
      "        allocation_description {\n",
      "          requested_bytes: 8\n",
      "          allocator_name: \"cpu\"\n",
      "          ptr: 140473296646560\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    timeline_label: \"random_uniform/shape = Const()\"\n",
      "    scheduled_micros: 1527656218975114\n",
      "    memory_stats {\n",
      "      persistent_memory_size: 8\n",
      "    }\n",
      "  }\n",
      "  node_stats {\n",
      "    node_name: \"random_uniform/RandomUniform\"\n",
      "    all_start_micros: 1527656218975123\n",
      "    op_start_rel_micros: 1\n",
      "    op_end_rel_micros: 3587\n",
      "    all_end_rel_micros: 3600\n",
      "    memory {\n",
      "      allocator_name: \"cpu\"\n",
      "      total_bytes: 16\n",
      "      peak_bytes: 16\n",
      "      live_bytes: 16\n",
      "      allocation_records {\n",
      "        alloc_micros: 1527656218975494\n",
      "        alloc_bytes: 16\n",
      "      }\n",
      "      allocation_records {\n",
      "        alloc_micros: 1527656218982368\n",
      "        alloc_bytes: -16\n",
      "      }\n",
      "    }\n",
      "    output {\n",
      "      tensor_description {\n",
      "        dtype: DT_FLOAT\n",
      "        shape {\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "        }\n",
      "        allocation_description {\n",
      "          requested_bytes: 16\n",
      "          allocated_bytes: 16\n",
      "          allocator_name: \"cpu\"\n",
      "          allocation_id: 1\n",
      "          has_single_reference: true\n",
      "          ptr: 140473296597408\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    timeline_label: \"random_uniform/RandomUniform = RandomUniform(random_uniform/shape)\"\n",
      "    scheduled_micros: 1527656218975121\n",
      "    memory_stats {\n",
      "    }\n",
      "  }\n",
      "  node_stats {\n",
      "    node_name: \"random_uniform/mul\"\n",
      "    all_start_micros: 1527656218978732\n",
      "    op_start_rel_micros: 2\n",
      "    op_end_rel_micros: 5\n",
      "    all_end_rel_micros: 8\n",
      "    memory {\n",
      "      allocator_name: \"cpu\"\n",
      "    }\n",
      "    output {\n",
      "      tensor_description {\n",
      "        dtype: DT_FLOAT\n",
      "        shape {\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "        }\n",
      "        allocation_description {\n",
      "          requested_bytes: 16\n",
      "          allocated_bytes: 16\n",
      "          allocator_name: \"cpu\"\n",
      "          allocation_id: 1\n",
      "          ptr: 140473296597408\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    timeline_label: \"random_uniform/mul = Snapshot(random_uniform/RandomUniform)\"\n",
      "    scheduled_micros: 1527656218978722\n",
      "    memory_stats {\n",
      "    }\n",
      "  }\n",
      "  node_stats {\n",
      "    node_name: \"random_uniform\"\n",
      "    all_start_micros: 1527656218978743\n",
      "    op_end_rel_micros: 1\n",
      "    all_end_rel_micros: 3\n",
      "    memory {\n",
      "      allocator_name: \"cpu\"\n",
      "    }\n",
      "    output {\n",
      "      tensor_description {\n",
      "        dtype: DT_FLOAT\n",
      "        shape {\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "        }\n",
      "        allocation_description {\n",
      "          requested_bytes: 16\n",
      "          allocated_bytes: 16\n",
      "          allocator_name: \"cpu\"\n",
      "          allocation_id: 1\n",
      "          ptr: 140473296597408\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    timeline_label: \"random_uniform = Snapshot(random_uniform/mul)\"\n",
      "    scheduled_micros: 1527656218978740\n",
      "    memory_stats {\n",
      "    }\n",
      "  }\n",
      "  node_stats {\n",
      "    node_name: \"MatMul\"\n",
      "    all_start_micros: 1527656218978748\n",
      "    op_end_rel_micros: 3606\n",
      "    all_end_rel_micros: 3622\n",
      "    memory {\n",
      "      allocator_name: \"cpu\"\n",
      "      total_bytes: 16\n",
      "      peak_bytes: 16\n",
      "      live_bytes: 16\n",
      "      allocation_records {\n",
      "        alloc_micros: 1527656218978753\n",
      "        alloc_bytes: 16\n",
      "      }\n",
      "    }\n",
      "    output {\n",
      "      tensor_description {\n",
      "        dtype: DT_FLOAT\n",
      "        shape {\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "          dim {\n",
      "            size: 2\n",
      "          }\n",
      "        }\n",
      "        allocation_description {\n",
      "          requested_bytes: 16\n",
      "          allocated_bytes: 16\n",
      "          allocator_name: \"cpu\"\n",
      "          allocation_id: 1\n",
      "          has_single_reference: true\n",
      "          ptr: 140473255053248\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    timeline_label: \"MatMul = MatMul(MatMul/a, random_uniform)\"\n",
      "    scheduled_micros: 1527656218978746\n",
      "    memory_stats {\n",
      "    }\n",
      "  }\n",
      "  node_stats {\n",
      "    node_name: \"_retval_MatMul_0_0\"\n",
      "    all_start_micros: 1527656218982377\n",
      "    op_start_rel_micros: 2\n",
      "    op_end_rel_micros: 6\n",
      "    all_end_rel_micros: 9\n",
      "    memory {\n",
      "      allocator_name: \"cpu\"\n",
      "    }\n",
      "    timeline_label: \"_retval_MatMul_0_0 = _Retval(MatMul)\"\n",
      "    scheduled_micros: 1527656218982370\n",
      "    memory_stats {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = tf.matmul([[37.0, -23.0], [1.0, 4.0]], tf.random_uniform([2, 2]))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Define options for the `sess.run()` call.\n",
    "    options = tf.RunOptions()\n",
    "    options.output_partition_graphs = True\n",
    "    options.trace_level = tf.RunOptions.FULL_TRACE\n",
    "\n",
    "    # Define a container for the returned metadata.\n",
    "    metadata = tf.RunMetadata()\n",
    "\n",
    "    sess.run(y, options=options, run_metadata=metadata)\n",
    "\n",
    "    # Print the subgraphs that executed on each device.\n",
    "    print(metadata.partition_graphs)\n",
    "\n",
    "    # Print the timings of each operation that executed.\n",
    "    print(metadata.step_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiple Graph\n",
    "- use **tf.Graph.as_default** to switch to different sub graph\n",
    "- use **tf.get_default_graph** to inspect the current graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_1 = tf.Graph()\n",
    "with g_1.as_default():\n",
    "  # Operations created in this scope will be added to `g_1`.\n",
    "  c = tf.constant(\"Node in g_1\")\n",
    "\n",
    "  # Sessions created in this scope will run operations from `g_1`.\n",
    "  sess_1 = tf.Session()\n",
    "\n",
    "g_2 = tf.Graph()\n",
    "with g_2.as_default():\n",
    "  # Operations created in this scope will be added to `g_2`.\n",
    "  d = tf.constant(\"Node in g_2\")\n",
    "\n",
    "# Alternatively, you can pass a graph when constructing a <a href=\"../api_docs/python/tf/Session\"><code>tf.Session</code></a>:\n",
    "# `sess_2` will run operations from `g_2`.\n",
    "sess_2 = tf.Session(graph=g_2)\n",
    "\n",
    "assert c.graph is g_1\n",
    "assert sess_1.graph is g_1\n",
    "\n",
    "assert d.graph is g_2\n",
    "assert sess_2.graph is g_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Operation 'Const' type=Const>, <tf.Operation 'other_variable' type=VariableV2>, <tf.Operation 'other_variable/Assign' type=Assign>, <tf.Operation 'other_variable/read' type=Identity>, <tf.Operation 'c' type=Const>, <tf.Operation 'c_1' type=Const>, <tf.Operation 'outer/c' type=Const>, <tf.Operation 'outer/inner/c' type=Const>, <tf.Operation 'outer/c_1' type=Const>, <tf.Operation 'outer/inner_1/c' type=Const>, <tf.Operation 'c_2' type=Const>, <tf.Operation 'c_3' type=Const>, <tf.Operation 'outer_1/c' type=Const>, <tf.Operation 'outer_1/inner/c' type=Const>, <tf.Operation 'outer_1/c_1' type=Const>, <tf.Operation 'outer_1/inner_1/c' type=Const>, <tf.Operation 'random_uniform/shape' type=Const>, <tf.Operation 'random_uniform/min' type=Const>, <tf.Operation 'random_uniform/max' type=Const>, <tf.Operation 'random_uniform/RandomUniform' type=RandomUniform>, <tf.Operation 'random_uniform/sub' type=Sub>, <tf.Operation 'random_uniform/mul' type=Mul>, <tf.Operation 'random_uniform' type=Add>, <tf.Operation 'MatMul/a' type=Const>, <tf.Operation 'MatMul' type=MatMul>]\n"
     ]
    }
   ],
   "source": [
    "# Print all of the operations in the default graph.\n",
    "g = tf.get_default_graph()\n",
    "print(g.get_operations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
